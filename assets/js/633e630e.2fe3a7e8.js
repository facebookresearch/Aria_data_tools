"use strict";(self.webpackChunkstaticdocs_starter=self.webpackChunkstaticdocs_starter||[]).push([[925],{3905:(e,t,r)=>{r.r(t),r.d(t,{MDXContext:()=>d,MDXProvider:()=>m,mdx:()=>y,useMDXComponents:()=>l,withMDXComponents:()=>p});var a=r(67294);function n(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(){return o=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var r=arguments[t];for(var a in r)Object.prototype.hasOwnProperty.call(r,a)&&(e[a]=r[a])}return e},o.apply(this,arguments)}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function s(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){n(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function c(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var d=a.createContext({}),p=function(e){return function(t){var r=l(t.components);return a.createElement(e,o({},t,{components:r}))}},l=function(e){var t=a.useContext(d),r=t;return e&&(r="function"==typeof e?e(t):s(s({},t),e)),r},m=function(e){var t=l(e.components);return a.createElement(d.Provider,{value:t},e.children)},u="mdxType",g={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},f=a.forwardRef((function(e,t){var r=e.components,n=e.mdxType,o=e.originalType,i=e.parentName,d=c(e,["components","mdxType","originalType","parentName"]),p=l(r),m=n,u=p["".concat(i,".").concat(m)]||p[m]||g[m]||o;return r?a.createElement(u,s(s({ref:t},d),{},{components:r})):a.createElement(u,s({ref:t},d))}));function y(e,t){var r=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var o=r.length,i=new Array(o);i[0]=f;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[u]="string"==typeof e?e:n,i[1]=s;for(var d=2;d<o;d++)i[d]=r[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,r)}f.displayName="MDXCreateElement"},38319:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>d,contentTitle:()=>s,default:()=>u,frontMatter:()=>i,metadata:()=>c,toc:()=>p});var a=r(87462),n=(r(67294),r(3905)),o=r(44996);const i={sidebar_position:6,id:"reprojected-gaze",title:"Eye Gaze Data"},s="Eye Gaze Data",c={unversionedId:"pilotdata/reprojected-gaze",id:"pilotdata/reprojected-gaze",title:"Eye Gaze Data",description:"Eye Gaze derived data uses data from Project Aria devices\u2019 eye tracking cameras to estimate where the user is looking. Data from the eye tracking cameras are used to estimate a vector expressing the gaze direction. That vector is translated into a 2D pixel coordinate on the 120 degree FOV RGB camera image.",source:"@site/docs/pilotdata/reprojected-gaze.mdx",sourceDirName:"pilotdata",slug:"/pilotdata/reprojected-gaze",permalink:"/Aria_data_tools/docs/pilotdata/reprojected-gaze",draft:!1,editUrl:"https://github.com/facebookresearch/aria_data_tools/docs/pilotdata/reprojected-gaze.mdx",tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6,id:"reprojected-gaze",title:"Eye Gaze Data"},sidebar:"tutorialSidebar",previous:{title:"Speech2Text Output Data",permalink:"/Aria_data_tools/docs/pilotdata/speech2text"},next:{title:"Timestamps Mapping Data",permalink:"/Aria_data_tools/docs/pilotdata/timestamps"}},d={},p=[],l={toc:p},m="wrapper";function u(e){let{components:t,...r}=e;return(0,n.mdx)(m,(0,a.Z)({},l,r,{components:t,mdxType:"MDXLayout"}),(0,n.mdx)("h1",{id:"eye-gaze-data"},"Eye Gaze Data"),(0,n.mdx)("p",null,"Eye Gaze derived data uses data from Project Aria devices\u2019 eye tracking cameras to estimate where the user is looking. Data from the eye tracking cameras are used to estimate a vector expressing the gaze direction. That vector is translated into a 2D pixel coordinate on the 120 degree FOV RGB camera image."),(0,n.mdx)("p",null,"The Eye Gaze Data for each recording is stored in ",(0,n.mdx)("inlineCode",{parentName:"p"},"eyetracking/et_in_rgb_stream.csv"),"."),(0,n.mdx)("p",null,"In the csv file (x,y) represents the calibrated 2D pixel coordinate of gaze position."),(0,n.mdx)("p",null,(0,n.mdx)("strong",{parentName:"p"},"Table 1:")," ",(0,n.mdx)("em",{parentName:"p"},(0,n.mdx)("inlineCode",{parentName:"em"},"et_in_rgb_stream.csv")," Structure")),(0,n.mdx)("div",null,(0,n.mdx)("table",null,(0,n.mdx)("tbody",null,(0,n.mdx)("tr",null,(0,n.mdx)("td",null,"timestamp_unix_ns"),(0,n.mdx)("td",{colSpan:2},"  Calibrated gaze position (pixel coordinate)")),(0,n.mdx)("tr",null,(0,n.mdx)("td",null," t "),(0,n.mdx)("td",null," x"),(0,n.mdx)("td",null," y"))))),(0,n.mdx)("admonition",{type:"note"},(0,n.mdx)("p",{parentName:"admonition"},"The provided timestamp is the timestamp of the eye camera image used to compute the gaze, not an RGB camera timestamp. To query the approximate location of eye gaze at RGB image timestamp, we suggest you interpolate gaze results from neighboring frames.")),(0,n.mdx)("video",{width:"950",controls:!0},(0,n.mdx)("source",{src:(0,o.default)("video/et_visualization.m4v"),type:"video/mp4"}),"Your browser does not support the video tag."),(0,n.mdx)("p",null,(0,n.mdx)("strong",{parentName:"p"},"Figure 1:")," ",(0,n.mdx)("em",{parentName:"p"},"Eye Tracking Visualization")))}u.isMDXComponent=!0}}]);