"use strict";(self.webpackChunkstaticdocs_starter=self.webpackChunkstaticdocs_starter||[]).push([[448],{3905:function(e,t,a){a.r(t),a.d(t,{MDXContext:function(){return d},MDXProvider:function(){return u},mdx:function(){return f},useMDXComponents:function(){return p},withMDXComponents:function(){return c}});var r=a(67294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(){return i=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var a=arguments[t];for(var r in a)Object.prototype.hasOwnProperty.call(a,r)&&(e[r]=a[r])}return e},i.apply(this,arguments)}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,r,o=function(e,t){if(null==e)return{};var a,r,o={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var d=r.createContext({}),c=function(e){return function(t){var a=p(t.components);return r.createElement(e,i({},t,{components:a}))}},p=function(e){var t=r.useContext(d),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},u=function(e){var t=p(e.components);return r.createElement(d.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},h=r.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,n=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),c=p(a),u=o,h=c["".concat(n,".").concat(u)]||c[u]||m[u]||i;return a?r.createElement(h,l(l({ref:t},d),{},{components:a})):r.createElement(h,l({ref:t},d))}));function f(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,n=new Array(i);n[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,n[1]=l;for(var d=2;d<i;d++)n[d]=a[d];return r.createElement.apply(null,n)}return r.createElement.apply(null,a)}h.displayName="MDXCreateElement"},85080:function(e,t,a){a.r(t),a.d(t,{assets:function(){return c},contentTitle:function(){return s},default:function(){return m},frontMatter:function(){return l},metadata:function(){return d},toc:function(){return p}});var r=a(83117),o=a(80102),i=(a(67294),a(3905)),n=["components"],l={sidebar_position:8,id:"mps",title:"Machine Perception Services"},s="Machine Perception Services",d={unversionedId:"howto/mps",id:"howto/mps",title:"Machine Perception Services",description:"This page provides information on how to use Machine Perception Services (MPS) derived data with Aria Data Tools. Currently, this tooling is only available in C++.",source:"@site/docs/howto/MPS.md",sourceDirName:"howto",slug:"/howto/mps",permalink:"/Aria_data_tools/docs/howto/mps",draft:!1,editUrl:"https://github.com/facebookresearch/aria_data_tools/docs/howto/MPS.md",tags:[],version:"current",sidebarPosition:8,frontMatter:{sidebar_position:8,id:"mps",title:"Machine Perception Services"},sidebar:"tutorialSidebar",previous:{title:"Using Calibration Sensor Data",permalink:"/Aria_data_tools/docs/howto/calibration"},next:{title:"Aria Pilot Dataset Overview",permalink:"/Aria_data_tools/docs/pilotdata/pilotdata-index"}},c={},p=[{value:"Aria Data Provider Code Snippets",id:"aria-data-provider-code-snippets",level:2},{value:"Read the data",id:"read-the-data",level:3},{value:"Online calibration",id:"online-calibration",level:3},{value:"Aria Trajectory Viewer",id:"aria-trajectory-viewer",level:2},{value:"Aria Data Provider Code Snippet",id:"aria-data-provider-code-snippet",level:2},{value:"Aria Eye Gaze Viewer",id:"aria-eye-gaze-viewer",level:2}],u={toc:p};function m(e){var t=e.components,l=(0,o.Z)(e,n);return(0,i.mdx)("wrapper",(0,r.Z)({},u,l,{components:t,mdxType:"MDXLayout"}),(0,i.mdx)("h1",{id:"machine-perception-services"},"Machine Perception Services"),(0,i.mdx)("h1",{id:"introduction"},"Introduction"),(0,i.mdx)("p",null,"This page provides information on how to use Machine Perception Services (MPS) derived data with Aria Data Tools. Currently, this tooling is only available in C++."),(0,i.mdx)("p",null,"MPS is a derived data service provided to Project Aria\u2019s Academic Research Partners. Approved research partners can request MPS to generate:"),(0,i.mdx)("ul",null,(0,i.mdx)("li",{parentName:"ul"},"Trajectory data - SLAM & IMU sensor data is used to get 6Dof pose trajectory, which can be used for downstream spatial AI tasks"),(0,i.mdx)("li",{parentName:"ul"},"Eye Gaze data -  Data from Aria\u2019s eye tracking (ET) cameras are used to estimate a vector expressing the gaze direction and uncertainty")),(0,i.mdx)("p",null,"If your organization would like to partner with Project Aria, gaining access to Aria devices and services such as MPS, please complete the ",(0,i.mdx)("a",{parentName:"p",href:"https://form.asana.com/?k=XOUA2BN-8ptiKPps2wLG3A&d=26764464567100"},"Partnership Interest Form"),"."),(0,i.mdx)("p",null,"To read and visualize MPS data, the ",(0,i.mdx)("a",{parentName:"p",href:"https://facebookresearch.github.io/Aria_data_tools/docs/howto/dataprovider/"},"Aria Data Provider")," (as of December 15 2022), now contains:"),(0,i.mdx)("ul",null,(0,i.mdx)("li",{parentName:"ul"},"Mps_io, a self contained library that allows it to parse MPS output files"),(0,i.mdx)("li",{parentName:"ul"},"Mps_visualization, visualization samples")),(0,i.mdx)("h1",{id:"trajectory-data"},"Trajectory data"),(0,i.mdx)("p",null,"SLAM & IMU data is used to get 6Dof pose trajectory, with open-loop trajectories, closed-loop trajectories and online-calibration data."),(0,i.mdx)("p",null,"Files used by the mps_io library:"),(0,i.mdx)("ul",null,(0,i.mdx)("li",{parentName:"ul"},"\u201cOpen_loop_trajectory.csv\u201d - The open loop trajectory of Aria device data is a locally smooth, gravity-aligned, 1kHz trajectory generated via visual inertial odometry (VIO) without loop closures. This trajectory format trades global accuracy for improved local accuracy."),(0,i.mdx)("li",{parentName:"ul"},"\u201cClosed_loop_trajectory.csv\u201d - The closed loop trajectory provides a gravity aligned, 1kHz trajectory with higher global accuracy compared to the ",(0,i.mdx)("inlineCode",{parentName:"li"},"open_loop_trajectory"),". This is achieved through loop closure."),(0,i.mdx)("li",{parentName:"ul"},"\u201cOnline_calibration.jsonl\u201d - ",(0,i.mdx)("a",{parentName:"li",href:"https://jsonlines.org/"},"JSONL")," files contain one-record-per-line of json-encoded records. Each record is a json dict object that contains timestamp metadata and the result of online calibration for the cameras and IMUs. The calibration parameters contain intrinsics/extrinsics parameters for each sensor as well as a time offsets which best temporally align their data.")),(0,i.mdx)("h2",{id:"aria-data-provider-code-snippets"},(0,i.mdx)("a",{parentName:"h2",href:"https://facebookresearch.github.io/Aria_data_tools/docs/howto/dataprovider/"},"Aria Data Provider")," Code Snippets"),(0,i.mdx)("h3",{id:"read-the-data"},"Read the data"),(0,i.mdx)("pre",null,(0,i.mdx)("code",{parentName:"pre"},'#include "trajectoryReader.h"\n\n# Depending of the input file, use the according function:\n\nstd::filesystem::path filepath = \u201c<PATH>/open_loop_trajectory.csv\u201d;\n\nTrajectory loadedOpenPose = readOpenLoop(locationFilePath);\n\nstd::filesystem::path filepath = \u201c<PATH>/closed_loop_trajectory.csv\u201d;\n\nTrajectory loadedClosePose = readCloseLoop(locationFilePath);\n')),(0,i.mdx)("h3",{id:"online-calibration"},"Online calibration"),(0,i.mdx)("pre",null,(0,i.mdx)("code",{parentName:"pre"},'#include "onlineCalibrationReader.h"\n\nstd::filesystem::path filepath = \u201c<PATH>/online_calibration.jsonl\u201d;\n\nTemporalDeviceModels readOnlineCalibration(filepath);\n')),(0,i.mdx)("h2",{id:"aria-trajectory-viewer"},"Aria Trajectory Viewer"),(0,i.mdx)("p",null,"Visualize trajectory data using AriaViewer"),(0,i.mdx)("pre",null,(0,i.mdx)("code",{parentName:"pre"},"$ cd build/visualization\n\n$ ./mps_trajectory_viewer <PATH>/closed_loop_trajectory.csv\n")),(0,i.mdx)("p",null,(0,i.mdx)("img",{alt:"image of trajectory viewer",src:a(37717).Z,width:"626",height:"508"})),(0,i.mdx)("h1",{id:"eye-gaze-data"},"Eye Gaze data"),(0,i.mdx)("p",null,"Eye Gaze data is produced by an MPS eye tracking algorithm. Gaze tracking is typically employed to determine a person's focus of attention (the direction they are looking in)."),(0,i.mdx)("p",null,"Files used by the mps_io library:"),(0,i.mdx)("ul",null,(0,i.mdx)("li",{parentName:"ul"},"\u201cEye-gaze-output.csv\u201c  provides the temporal state of the EyeGaze vector and uncertainty"),(0,i.mdx)("li",{parentName:"ul"},"\u201cEye-gaze-coord-transform.json\u201d provides the transform from the device to CPF (Central Pupil Frame)")),(0,i.mdx)("h2",{id:"aria-data-provider-code-snippet"},(0,i.mdx)("a",{parentName:"h2",href:"https://facebookresearch.github.io/Aria_data_tools/docs/howto/dataprovider/"},"Aria Data Provider")," Code Snippet"),(0,i.mdx)("pre",null,(0,i.mdx)("code",{parentName:"pre"},'#include "eyeGazeReader.h"\n\nstd::filesystem::path filepath = \u201c<PATH>/eye-gaze-output.csv\u201d;\n\nTemporalEyeGazeData eyeGaze_records = readEyeGaze(filepath);\n')),(0,i.mdx)("p",null,"Notes on how Eye Gaze data aligns with VRS image data:"),(0,i.mdx)("ul",null,(0,i.mdx)("li",{parentName:"ul"},"EyeGaze data is timestamp based (close to EyeCamera image timestamps)."),(0,i.mdx)("li",{parentName:"ul"},"To query the approximate location of eye gaze at any given timestamp (i.e RGB image timestamp), you can use the ",(0,i.mdx)("inlineCode",{parentName:"li"},"QueryEyeGaze")," function, which demonstrates how to interpolate EyeGaze vector data.")),(0,i.mdx)("h2",{id:"aria-eye-gaze-viewer"},"Aria Eye Gaze Viewer"),(0,i.mdx)("p",null,"Visualize Eye Gaze data using ",(0,i.mdx)("a",{parentName:"p",href:"https://facebookresearch.github.io/Aria_data_tools/docs/howto/visualizing/"},"AriaViewer"),"."),(0,i.mdx)("pre",null,(0,i.mdx)("code",{parentName:"pre"},"$ cd build/visualization\n\n$ ./mps_eyegaze_viewer <PATH>GUID.vrs <PATH>/eye-gaze-output.csv\n")),(0,i.mdx)("p",null,(0,i.mdx)("img",{alt:"image of eye gaze viewer",src:a(97527).Z,width:"881",height:"828"})))}m.isMDXComponent=!0},97527:function(e,t,a){t.Z=a.p+"assets/images/EyeGazeViewer-1b20ab6fd4178fd16736aa8bf914e7df.png"},37717:function(e,t,a){t.Z=a.p+"assets/images/TrajectoryViewer-0344b113f477dbbedf4c36f6665a67e1.png"}}]);