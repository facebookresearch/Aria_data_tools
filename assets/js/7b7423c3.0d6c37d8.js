"use strict";(self.webpackChunkstaticdocs_starter=self.webpackChunkstaticdocs_starter||[]).push([[134],{3905:(e,a,t)=>{t.r(a),t.d(a,{MDXContext:()=>d,MDXProvider:()=>m,mdx:()=>f,useMDXComponents:()=>p,withMDXComponents:()=>c});var r=t(67294);function o(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(){return i=Object.assign||function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var r in t)Object.prototype.hasOwnProperty.call(t,r)&&(e[r]=t[r])}return e},i.apply(this,arguments)}function n(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function s(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?n(Object(t),!0).forEach((function(a){o(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):n(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,r,o=function(e,a){if(null==e)return{};var t,r,o={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||(o[t]=e[t]);return o}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var d=r.createContext({}),c=function(e){return function(a){var t=p(a.components);return r.createElement(e,i({},a,{components:t}))}},p=function(e){var a=r.useContext(d),t=a;return e&&(t="function"==typeof e?e(a):s(s({},a),e)),t},m=function(e){var a=p(e.components);return r.createElement(d.Provider,{value:a},e.children)},u="mdxType",h={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},y=r.forwardRef((function(e,a){var t=e.components,o=e.mdxType,i=e.originalType,n=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),c=p(t),m=o,u=c["".concat(n,".").concat(m)]||c[m]||h[m]||i;return t?r.createElement(u,s(s({ref:a},d),{},{components:t})):r.createElement(u,s({ref:a},d))}));function f(e,a){var t=arguments,o=a&&a.mdxType;if("string"==typeof e||o){var i=t.length,n=new Array(i);n[0]=y;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[u]="string"==typeof e?e:o,n[1]=s;for(var d=2;d<i;d++)n[d]=t[d];return r.createElement.apply(null,n)}return r.createElement.apply(null,t)}y.displayName="MDXCreateElement"},24179:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>n,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var r=t(87462),o=(t(67294),t(3905));const i={sidebar_position:8,id:"mps",title:"Machine Perception Services"},n="Machine Perception Services",s={unversionedId:"howto/mps",id:"howto/mps",title:"Machine Perception Services",description:"These instructions are for Aria Data Tools, which is used for the Aria Pilot Dataset. If you are working with any other Aria data, we recommend using Project Aria Tools instead. Go to Aria Machine Perception Services in Project Aria Tools for more information and details about MPS.",source:"@site/docs/howto/MPS.mdx",sourceDirName:"howto",slug:"/howto/mps",permalink:"/Aria_data_tools/docs/howto/mps",draft:!1,editUrl:"https://github.com/facebookresearch/aria_data_tools/docs/howto/MPS.mdx",tags:[],version:"current",sidebarPosition:8,frontMatter:{sidebar_position:8,id:"mps",title:"Machine Perception Services"},sidebar:"tutorialSidebar",previous:{title:"Using Calibration Sensor Data",permalink:"/Aria_data_tools/docs/howto/calibration"},next:{title:"Aria Pilot Dataset Overview",permalink:"/Aria_data_tools/docs/pilotdata/pilotdata-index"}},l={},d=[{value:"Aria Data Provider Code Snippets",id:"aria-data-provider-code-snippets",level:2},{value:"Read the data",id:"read-the-data",level:3},{value:"Online calibration",id:"online-calibration",level:3},{value:"Aria Trajectory Viewer",id:"aria-trajectory-viewer",level:2},{value:"Aria Data Provider Code Snippet",id:"aria-data-provider-code-snippet",level:2},{value:"Aria Eye Gaze Viewer",id:"aria-eye-gaze-viewer",level:2}],c={toc:d},p="wrapper";function m(e){let{components:a,...i}=e;return(0,o.mdx)(p,(0,r.Z)({},c,i,{components:a,mdxType:"MDXLayout"}),(0,o.mdx)("h1",{id:"machine-perception-services"},"Machine Perception Services"),(0,o.mdx)("admonition",{type:"caution"},(0,o.mdx)("p",{parentName:"admonition"},"These instructions are for Aria Data Tools, which is used for the Aria Pilot Dataset. If you are working with any other Aria data, we recommend using ",(0,o.mdx)("a",{parentName:"p",href:"https://facebookresearch.github.io/projectaria_tools/docs/intro"},"Project Aria Tools")," instead. Go to ",(0,o.mdx)("a",{parentName:"p",href:"https://facebookresearch.github.io/projectaria_tools/docs/ARK/mps"},"Aria Machine Perception Services")," in Project Aria Tools for more information and details about MPS.")),(0,o.mdx)("h1",{id:"introduction"},"Introduction"),(0,o.mdx)("p",null,"This page provides information on how to use Machine Perception Services (MPS) derived data with Aria Data Tools. Currently, this tooling is only available in C++."),(0,o.mdx)("p",null,"MPS is a derived data service provided to Project Aria\u2019s Academic Research Partners. Approved research partners can request MPS to generate:"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},"Trajectory data - SLAM & IMU sensor data is used to get 6Dof pose trajectory, which can be used for downstream spatial AI tasks"),(0,o.mdx)("li",{parentName:"ul"},"Eye Gaze data -  Data from Aria\u2019s eye tracking (ET) cameras are used to estimate a vector expressing the gaze direction and uncertainty")),(0,o.mdx)("p",null,"If your organization would like to partner with Project Aria, gaining access to Aria devices and services such as MPS, please complete the ",(0,o.mdx)("a",{parentName:"p",href:"https://form.asana.com/?k=XOUA2BN-8ptiKPps2wLG3A&d=26764464567100"},"Partnership Interest Form"),"."),(0,o.mdx)("p",null,"To read and visualize MPS data, the ",(0,o.mdx)("a",{parentName:"p",href:"https://facebookresearch.github.io/Aria_data_tools/docs/howto/dataprovider/"},"Aria Data Provider")," (as of December 15 2022), now contains:"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},"Mps_io, a self contained library that allows it to parse MPS output files"),(0,o.mdx)("li",{parentName:"ul"},"Mps_visualization, visualization samples")),(0,o.mdx)("h1",{id:"trajectory-data"},"Trajectory data"),(0,o.mdx)("p",null,"SLAM & IMU data is used to get 6Dof pose trajectory, with open-loop trajectories, closed-loop trajectories and online-calibration data."),(0,o.mdx)("p",null,"Files used by the mps_io library:"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},"\u201cOpen_loop_trajectory.csv\u201d - The open loop trajectory of Aria device data is a locally smooth, gravity-aligned, 1kHz trajectory generated via visual inertial odometry (VIO) without loop closures. This trajectory format trades global accuracy for improved local accuracy."),(0,o.mdx)("li",{parentName:"ul"},"\u201cClosed_loop_trajectory.csv\u201d - The closed loop trajectory provides a gravity aligned, 1kHz trajectory with higher global accuracy compared to the ",(0,o.mdx)("inlineCode",{parentName:"li"},"open_loop_trajectory"),". This is achieved through loop closure."),(0,o.mdx)("li",{parentName:"ul"},"\u201cOnline_calibration.jsonl\u201d - ",(0,o.mdx)("a",{parentName:"li",href:"https://jsonlines.org/"},"JSONL")," files contain one-record-per-line of json-encoded records. Each record is a json dict object that contains timestamp metadata and the result of online calibration for the cameras and IMUs. The calibration parameters contain intrinsics/extrinsics parameters for each sensor as well as a time offsets which best temporally align their data.")),(0,o.mdx)("h2",{id:"aria-data-provider-code-snippets"},(0,o.mdx)("a",{parentName:"h2",href:"https://facebookresearch.github.io/Aria_data_tools/docs/howto/dataprovider/"},"Aria Data Provider")," Code Snippets"),(0,o.mdx)("h3",{id:"read-the-data"},"Read the data"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},'#include "trajectoryReader.h"\n\n# Depending of the input file, use the according function:\n\nstd::filesystem::path filepath = \u201c<PATH>/open_loop_trajectory.csv\u201d;\n\nTrajectory loadedOpenPose = readOpenLoop(locationFilePath);\n\nstd::filesystem::path filepath = \u201c<PATH>/closed_loop_trajectory.csv\u201d;\n\nTrajectory loadedClosePose = readCloseLoop(locationFilePath);\n')),(0,o.mdx)("h3",{id:"online-calibration"},"Online calibration"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},'#include "onlineCalibrationReader.h"\n\nstd::filesystem::path filepath = \u201c<PATH>/online_calibration.jsonl\u201d;\n\nTemporalDeviceModels readOnlineCalibration(filepath);\n')),(0,o.mdx)("h2",{id:"aria-trajectory-viewer"},"Aria Trajectory Viewer"),(0,o.mdx)("p",null,"Visualize trajectory data using AriaViewer"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"$ cd build/visualization\n\n$ ./mps_trajectory_viewer <PATH>/closed_loop_trajectory.csv\n")),(0,o.mdx)("p",null,(0,o.mdx)("img",{alt:"image of trajectory viewer",src:t(37717).Z,width:"626",height:"508"})),(0,o.mdx)("p",null,"Note: You can also add eye gaze vector data on top of the device trajectory"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"$ cd build/visualization\n\n$ ./mps_trajectory_viewer -- <PATH>/closed_loop_trajectory.csv <PATH>/eye-gaze-output.csv\n")),(0,o.mdx)("p",null,(0,o.mdx)("img",{alt:"image of trajectory viewer",src:t(60358).Z,width:"1922",height:"1050"})),(0,o.mdx)("h1",{id:"eye-gaze-data"},"Eye Gaze data"),(0,o.mdx)("p",null,"Eye Gaze data is produced by an MPS eye tracking algorithm. Gaze tracking is typically employed to determine a person's focus of attention (the direction they are looking in)."),(0,o.mdx)("p",null,"Files used by the mps_io library:"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},"\u201cEye-gaze-output.csv\u201c  provides the temporal state of the EyeGaze vector and uncertainty"),(0,o.mdx)("li",{parentName:"ul"},"\u201cEye-gaze-coord-transform.json\u201d provides the transform from the device to CPF (Central Pupil Frame)")),(0,o.mdx)("h2",{id:"aria-data-provider-code-snippet"},(0,o.mdx)("a",{parentName:"h2",href:"https://facebookresearch.github.io/Aria_data_tools/docs/howto/dataprovider/"},"Aria Data Provider")," Code Snippet"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},'#include "eyeGazeReader.h"\n\nstd::filesystem::path filepath = \u201c<PATH>/eye-gaze-output.csv\u201d;\n\nTemporalEyeGazeData eyeGaze_records = readEyeGaze(filepath);\n')),(0,o.mdx)("p",null,"Notes on how Eye Gaze data aligns with VRS image data:"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},"EyeGaze data is timestamp based (close to EyeCamera image timestamps)."),(0,o.mdx)("li",{parentName:"ul"},"To query the approximate location of eye gaze at any given timestamp (i.e RGB image timestamp), you can use the ",(0,o.mdx)("inlineCode",{parentName:"li"},"QueryEyeGaze")," function, which demonstrates how to interpolate EyeGaze vector data.")),(0,o.mdx)("h2",{id:"aria-eye-gaze-viewer"},"Aria Eye Gaze Viewer"),(0,o.mdx)("p",null,"Visualize Eye Gaze data using ",(0,o.mdx)("a",{parentName:"p",href:"https://facebookresearch.github.io/Aria_data_tools/docs/howto/visualizing/"},"AriaViewer"),"."),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre"},"$ cd build/visualization\n\n$ ./mps_eyegaze_viewer <PATH>GUID.vrs <PATH>/eye-gaze-output.csv\n")),(0,o.mdx)("p",null,(0,o.mdx)("img",{alt:"image of eye gaze viewer",src:t(97527).Z,width:"881",height:"828"})))}m.isMDXComponent=!0},97527:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/EyeGazeViewer-1b20ab6fd4178fd16736aa8bf914e7df.png"},60358:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/TrajectoryAndEyeGazeVectorsViewer-074ff36a11ec853bc8484f9c1fc8cf66.png"},37717:(e,a,t)=>{t.d(a,{Z:()=>r});const r=t.p+"assets/images/TrajectoryViewer-0344b113f477dbbedf4c36f6665a67e1.png"}}]);