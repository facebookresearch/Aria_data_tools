"use strict";(self.webpackChunkstaticdocs_starter=self.webpackChunkstaticdocs_starter||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Overview","href":"/Aria_data_tools/docs/overview","docId":"overview"},{"type":"link","label":"Install","href":"/Aria_data_tools/docs/Install","docId":"Install"},{"type":"link","label":"Sensors and Measurements","href":"/Aria_data_tools/docs/sensors-measurements","docId":"sensors-measurements"},{"type":"category","label":"How to Use the Tools","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Examples","href":"/Aria_data_tools/docs/howto/examples","docId":"howto/examples"},{"type":"link","label":"Accessing Sensor Data","href":"/Aria_data_tools/docs/howto/dataprovider","docId":"howto/dataprovider"},{"type":"link","label":"Visualize Sequences and Pre-Computed Camera Trajectory","href":"/Aria_data_tools/docs/howto/visualizing","docId":"howto/visualizing"},{"type":"link","label":"Using Calibration Sensor Data","href":"/Aria_data_tools/docs/howto/calibration","docId":"howto/calibration"},{"type":"link","label":"Machine Perception Services","href":"/Aria_data_tools/docs/howto/mps","docId":"howto/mps"}]},{"type":"category","label":"Aria Pilot Dataset","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Aria Pilot Dataset Overview","href":"/Aria_data_tools/docs/pilotdata/pilotdata-index","docId":"pilotdata/pilotdata-index"},{"type":"link","label":"Location Output Data","href":"/Aria_data_tools/docs/pilotdata/location-output","docId":"pilotdata/location-output"},{"type":"link","label":"Speech2Text Output Data","href":"/Aria_data_tools/docs/pilotdata/speech2text","docId":"pilotdata/speech2text"},{"type":"link","label":"Eye Gaze Data","href":"/Aria_data_tools/docs/pilotdata/reprojected-gaze","docId":"pilotdata/reprojected-gaze"},{"type":"link","label":"Timestamps Mapping Data","href":"/Aria_data_tools/docs/pilotdata/timestamps","docId":"pilotdata/timestamps"},{"type":"category","label":"Everyday Activities","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Everyday Activities Sensor Profile","href":"/Aria_data_tools/docs/pilotdata/everyday/sensor-profiles","docId":"pilotdata/everyday/sensor-profiles"},{"type":"link","label":"Activities","href":"/Aria_data_tools/docs/pilotdata/everyday/activities","docId":"pilotdata/everyday/activities"},{"type":"link","label":"Recording Scripts","href":"/Aria_data_tools/docs/pilotdata/everyday/pilotdata-scripts","docId":"pilotdata/everyday/pilotdata-scripts"}],"href":"/Aria_data_tools/docs/pilotdata/everyday/"},{"type":"category","label":"Desktop Activities Multi-View","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Desktop Activities Overview","href":"/Aria_data_tools/docs/pilotdata/desk/desktop_overview","docId":"pilotdata/desk/desktop_overview"},{"type":"link","label":"Desktop Activities Capture Setup","href":"/Aria_data_tools/docs/pilotdata/desk/desktop_setup","docId":"pilotdata/desk/desktop_setup"}]}]},{"type":"link","label":"How Project Aria Uses VRS","href":"/Aria_data_tools/docs/aria-vrs","docId":"aria-vrs"},{"type":"link","label":"Getting to Know and Use VRS","href":"/Aria_data_tools/docs/use-vrs","docId":"use-vrs"},{"type":"link","label":"FAQ","href":"/Aria_data_tools/docs/faq","docId":"faq"},{"type":"link","label":"Citation and Contributing","href":"/Aria_data_tools/docs/citation-contribute","docId":"citation-contribute"},{"type":"link","label":"License","href":"/Aria_data_tools/docs/license","docId":"license"}]},"docs":{"aria-vrs":{"id":"aria-vrs","title":"How Project Aria Uses VRS","description":"Project Aria chose VRS as its data container because it is a file format designed to record and playback streams of XR sensor data and supports huge file sizes. These VRS files contain streams of time-sorted records generated for each sensor, with one set of sensors per stream. Project Aria data uses VRS for features such as:","sidebar":"tutorialSidebar"},"citation-contribute":{"id":"citation-contribute","title":"Citation and Contributing","description":"Aria Research Kit: Aria Data Tools  and Aria Pilot Dataset White Paper","sidebar":"tutorialSidebar"},"faq":{"id":"faq","title":"FAQ","description":"Installation","sidebar":"tutorialSidebar"},"howto/calibration":{"id":"howto/calibration","title":"Using Calibration Sensor Data","description":"Introduction","sidebar":"tutorialSidebar"},"howto/dataprovider":{"id":"howto/dataprovider","title":"Accessing Sensor Data","description":"Introduction","sidebar":"tutorialSidebar"},"howto/examples":{"id":"howto/examples","title":"Examples","description":"Introduction","sidebar":"tutorialSidebar"},"howto/mps":{"id":"howto/mps","title":"Machine Perception Services","description":"This page provides information on how to use Machine Perception Services (MPS) derived data with Aria Data Tools. Currently, this tooling is only available in C++.","sidebar":"tutorialSidebar"},"howto/visualizing":{"id":"howto/visualizing","title":"Visualize Sequences and Pre-Computed Camera Trajectory","description":"Introduction","sidebar":"tutorialSidebar"},"Install":{"id":"Install","title":"Install","description":"What is it?","sidebar":"tutorialSidebar"},"license":{"id":"license","title":"License","description":"Aria Data Tools are released by Meta under the Apache 2.0 license.","sidebar":"tutorialSidebar"},"overview":{"id":"overview","title":"Overview","description":"Project Aria makes open data and open tooling available to support researchers expand the horizons of Augmented Reality, Machine Perception and Artificial Intelligence by releasing the Aria Pilot Dataset and Aria Research Kit: Aria Data Tools.","sidebar":"tutorialSidebar"},"pilotdata/desk/desktop_overview":{"id":"pilotdata/desk/desktop_overview","title":"Desktop Activities Overview","description":"Desktop Activities recorded a single operator manipulating objects over a desktop space. Data was captured by the operator wearing a Project Aria device and by a multi-view motion capture system. Most of the objects manipulated were 10 commonly used objects from the YCB Benchmark object set.","sidebar":"tutorialSidebar"},"pilotdata/desk/desktop_setup":{"id":"pilotdata/desk/desktop_setup","title":"Desktop Activities Capture Setup","description":"The Desktop Activities dataset was captured with a Project Aria device and a multi-view motion capture system.","sidebar":"tutorialSidebar"},"pilotdata/everyday/activities":{"id":"pilotdata/everyday/activities","title":"Activities","description":"When creating recordings for one to two Project Aria device wearers, we created scripts to represent all day long activities with always on sensing. Each script contained multiple scenarios that told a story about people going through their day. The scripts provided general guidance for an improvised scenario. Actors did not have specific lines to learn, instead they followed prompts and went with what felt most natural to to them.","sidebar":"tutorialSidebar"},"pilotdata/everyday/Everyday Activities":{"id":"pilotdata/everyday/Everyday Activities","title":"Everyday Activities File Structure","description":"The Everyday Activities dataset contains multiple activity sequences for one to two Project Aria device users. We created recordings using scripts to represent all day activities with always on sensing.","sidebar":"tutorialSidebar"},"pilotdata/everyday/pilotdata-scripts":{"id":"pilotdata/everyday/pilotdata-scripts","title":"Recording Scripts","description":"The following scripts were used by actors to collect data for Everyday Activities.","sidebar":"tutorialSidebar"},"pilotdata/everyday/sensor-profiles":{"id":"pilotdata/everyday/sensor-profiles","title":"Everyday Activities Sensor Profile","description":"Sensor profiles allow researchers to choose which sensors on the Project Aria device to use when collecting data. Sensor Profile 9 was used for the Everyday Activities dataset.","sidebar":"tutorialSidebar"},"pilotdata/location-output":{"id":"pilotdata/location-output","title":"Location Output Data","description":"Location Output Data shows the Project Aria device\u2019s position and orientation in the world across six dimensions of freedom.","sidebar":"tutorialSidebar"},"pilotdata/pilotdata-index":{"id":"pilotdata/pilotdata-index","title":"Aria Pilot Dataset Overview","description":"The Aria Pilot dataset is the first open dataset captured using Project Aria, Meta\u2019s research device used for accelerating machine perception and AI research, developed at Reality-Labs Research.","sidebar":"tutorialSidebar"},"pilotdata/reprojected-gaze":{"id":"pilotdata/reprojected-gaze","title":"Eye Gaze Data","description":"Eye Gaze derived data uses data from Project Aria devices\u2019 eye tracking cameras to estimate where the user is looking. Data from the eye tracking cameras are used to estimate a vector expressing the gaze direction. That vector is translated into a 2D pixel coordinate on the 120 degree FOV RGB camera image.","sidebar":"tutorialSidebar"},"pilotdata/speech2text":{"id":"pilotdata/speech2text","title":"Speech2Text Output Data","description":"Speech2Text Output Data provides text strings generated by Automatic Speech Recognition with timestamps and confidence rating.","sidebar":"tutorialSidebar"},"pilotdata/timestamps":{"id":"pilotdata/timestamps","title":"Timestamps Mapping Data","description":"Project Aria devices and multi-view devices operating in proximity to each other (<100m) can leverage SMPTE timecode to receive a synchronized time clock with sub-millisecond accuracy.","sidebar":"tutorialSidebar"},"sensors-measurements":{"id":"sensors-measurements","title":"Sensors and Measurements","description":"Introduction","sidebar":"tutorialSidebar"},"use-vrs":{"id":"use-vrs","title":"Getting to Know and Use VRS","description":"Introduction","sidebar":"tutorialSidebar"}}}')}}]);